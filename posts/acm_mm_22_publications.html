<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google-site-verification" content="" />
    <title>Buzzes, vocalisations and faces - Moreno La Quatra</title>

    <meta name="description" content="Moreno La Quatra personal website" />
    <meta name="keywords" content="attending conference, ACM Multimedia, 2022, blog" />
    <meta property="og:title" content="ACM Multimedia 2022" />
    <meta property="og:url" content="https://mlaquatra.me/posts/acm_mm_22_publications.html" />
    <meta property="og:description" content="Moreno La Quatra personal website" />
    <meta property="og:type" content="website" />
    <link rel="stylesheet" href="../src/style.css" />
    <link rel="stylesheet" href="../prism/prism.css" />
    <script src="../src/jquery-3.6.0.min.js"></script>
</head>

<body id="top" class="libertinus">
    <div data-include="header-nested-1"></div>
    <header>
        <h1><span class="latex">Buzzes, vocalisations and faces</h1>
    </header>

    <div class="abstract">
        <p>
            ACM Multimedia 2022 is a few weeks away and, with my colleagues from Politecnico di Torino, we are setting up the final touches to our presentations.
            We had the pleasure to work on very interesting research problems during these last months, and we are excited to present some of our findings in Lisboa.
            This post contains a short preview of what we are going to present.

        </p>
    </div>

    <nav role="navigation" class="toc">
        <ol>
            <li><a href="#buzzes">Mosquito detection</a></li>
            <li><a href="#vocalisations">Vocalisations</a></li>
            <li><a href="#faces">Emotion in your faces (?)</a></li>
        </ol>
    </nav>

    <h4 id="buzzes">How Much Attention Should we Pay to Mosquitoes?</h4>
    <p>
        We all know how annoying mosquitoes can be, but they can also transmit diseases such as malaria, dengue, and Zika. Interested in audio processing, we decided to take part in the Mosquito detection challenge at <a href="http://www.compare.openaudio.eu/2022-2/">ComParE 2022</a>. It is one of the grand challenges organized within the ACM Multimedia conference, and the goal is to develop automatic systems that can detect the presence of mosquitoes in audio recordings.
    </p>

    <p>
        We used a novel transformer-based approach and framed the problem as an audio frame classification task. Our model leverages the Wav2Vec 2.0<label for="sn-1" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33, 12449-12460. <a href="https://arxiv.org/abs/2006.11477">On arXiv</a>.
        </span> pre-trained model and predicts whether each frame contains mosquito sounds or not. Looking at our NLP background, we propose different labeling schemes to better exploit the context information within the audio recordings.
            </p>
            <br>
            <br>
            <img src="resources/acm_mm_22/mosquitoes_labeling_schemes.png" loading="lazy" alt="Sequence labeling schemes">
            <figcaption>
                Overview of the sequence labeling schemes proposed in the paper.
            </figcaption>
            <br>

            <p>
                A few takeaways from the results:
                <ul>
                    <li>
                        <b>Contextual information</b>: experimenting with different context windows, we found that the model is able to leverage the contextual information in the audio recordings to improve the performance. However, longer context windows
                        may introduce noisy or irrelevant information, which can negatively affect the performance.
                    </li>
                    <li>
                        <b>Multi-class labeling scheme</b>: the proposed labeling schemes allows the system to better model the temporal dependencies between the audio frames. In particular, the multi-class labeling scheme outperforms the binary labeling
                        scheme in all cases.
                    </li>
                    <li>
                        <b>Pitfall in evaluation metrics</b>: thanks to the organizers, we found that our model tends to predict short-lasting backgrounds within long-lasting mosquito events. Not all evaluation metrics are able to capture this behavior,
                        and we aim at investigating this issue in the future.
                    </li>
                </ul>
            </p>

            <h4 id="vocalisations">Transformer-based Non-Verbal Emotion Recognition: Exploring Model Portability across Speakers’ Genders</h4>
            <p>
                A considerable body of research in the field of automatic emotion recognition from speech has been developed in the last years. However, most of the existing systems are designed and trained on a single dataset, usually collected from a specific geographical
                region, and are not generalizable to other datasets.
            </p>

            <p>
                In this work, we address the vocalisation challenge at <a href="http://www.compare.openaudio.eu/2022-2/">ComParE 2022</a>. It aims at building systems that automatically recognise emotions from short vocalisations. The peculiarity of this
                challenge is that the training dataset is composed of female-only recordings while the test set includes male-only recordings.
            </p>

            <p>
                To address the challenge, we propose a transformer-based emotion recognition system that is trained on the challenge’s training set. However, to allow for model portability across different speakers' genders, we augment the training set by using both
                traditional and neural data augmentation techniques.
                <ul>
                    <li><b>Pitch shifting</b>: we shift the pitch of the audio recordings to alter the mean fundamental frequency of phonation (F0) and the formant frequencies. This technique aims at increasing the variability of the training set and, in
                        turn, the generalization capabilities of the model.</li>
                    <li><b>Neural augmentation</b>: A neural model<label for="sn-2" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Lin, J. H., Lin, Y. Y., Chien, C. M., & Lee, H. Y. (2021). S2vc: a framework for any-to-any voice conversion with self-supervised pretrained representations. arXiv preprint arXiv:2104.02901. <a href="https://arxiv.org/abs/2104.02901">On arXiv</a>.</span>                        is trained to generate synthetic audio recordings and augment the training set.</li>
                </ul>
                By using specific pre-training objective, we first adapt the model using contrastive learning objectives and then fine-tune it for the emotion recognition task.
            </p>





            <h4 id="faces">ViPER: Video-based Perceiver for Emotion Recognition</h4>
            <p>
                Capturing emotions from facial expressions is a challenging task. In this work, proposed for the <a href="https://www.muse-challenge.org/">MuSe 2022 workshop</a>, we propose a novel video-based emotion recognition system that leverages
                the Perceiver<label for="sn-3" class="sidenote-toggle sidenote-number"></label>
                <span class="sidenote">Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., & Carreira, J. (2021, July). Perceiver: General perception with iterative attention. In International conference on machine learning (pp. 4651-4664). PMLR.
                    <a href="https://arxiv.org/abs/2103.03206">On arXiv</a>.</span> architecture. It is a modality-agnostic architecture that can be used to process different types of data, such as images, videos, audio, and text.
            </p>

            <p>
                The solution aims at identifying the emotions expressed by a person reacting to a video clip. The input to the model is a video clip and the output is a vector of probabilities that represent the likelihood of each emotion class. We use the Perceiver
                to process multiple modalities at the same time. We experimented with different combinations of modalities:
                <ul>
                    <li>
                        <b>Image embeddings</b>: we use ViT<label for="sn-4" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020, September). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations.
                            <a href="https://arxiv.org/abs/2010.11929">On arXiv</a>.</span> to extract image embeddings from the video frames.
                    </li>
                    <li>
                        <b>Audio Embeddings</b>: we use WavLM<label for="sn-5" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., ... & Wei, F. (2022). Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing.<a href="https://arxiv.org/abs/2110.13900">On arXiv</a>.</span>                        to extract audio embeddings from the video frames.
                    </li>
                    <li>
                        <b>Text embeddings</b>: leveraging a set of manually defined templates, we extract face captions from the video frames and use a RoBERTa<label for="sn-6" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Liu, Y., Goyal, N., Gupta, P., Levy, O., & Rush, A. M. (2020). RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692. <a href="https://arxiv.org/abs/1907.11692">On arXiv</a>.</span>                        model to extract text embeddings.
                    </li>
                    <li>
                        <b>FAUs</b>: we use as features the Facial Action Units (FAUs) extracted from the video frames.
                    </li>
                </ul>
            </p>
            <img src="resources/acm_mm_22/ViPER.png" loading="lazy" alt="Sequence labeling schemes">
            <figcaption>
                Overview of ViPER architecture.
            </figcaption>
            <p>
                The results shows that combining multiple modalities improves the performance of the model. As expected, the audio modality is the least discriminative, while the FAUs and image embeddings are the most discriminative. Besides being derived from a set
                of manually-defined templates, the text embeddings shown to be effective for improving the performance of the model.
            </p>

            <h4 id="wrap-up">Wrap-up</h4>
            <p>
                This post describes three papers that will be presented at the ACM Multimedia 2022 conference. Each of them address a different challenge in the field of audio or multimodal processing. Complete information could be found in the papers that will be linked
                as soon as they are available. The details of each methodology are described in the papers, while this post provides only a high-level overview of the proposed solutions.
            </p>
            <p>
                I want to thank my co-authors <i>Lorenzo Vaiani, Alkis Koudounas, Luca Cagliero, Paolo Garza and Elena Baralis</i> for their contributions to this works.
            </p>


            <script>
                MathJax = {
                    tex: {
                        inlineMath: [
                            ['$', '$'],
                        ],
                    },
                }
            </script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            <script async src="../prism/prism.js"></script>

            <script async defer data-domain="latex.now.sh" src="https://plausible.io/js/plausible.js"></script>
            <!-- JavaScript Bundle with Popper -->
            <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-A3rJD856KowSb7dwlZdYEkO39Gagi7vIsF0jrRAoQmDKKtQBHUuLZ9AsSv4jD4Xa" crossorigin="anonymous"></script>

            <div data-include="footer"></div>

            <script>
                $(function() {
                    var includes = $('[data-include]')
                    $.each(includes, function() {
                        var file = '../components/' + $(this).data('include') + '.html'
                        $(this).load(file)
                    })
                })
            </script>


</body>

</html>