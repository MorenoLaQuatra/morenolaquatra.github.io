<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google-site-verification" content="" />
    <title>ARCH - Moreno La Quatra</title>

    <meta name="description" content="Moreno La Quatra personal website" />
    <meta name="keywords" content="attending conference, ACM Multimedia, 2022, blog" />
    <meta property="og:title" content="ACM Multimedia 2022" />
    <meta property="og:url" content="https://mlaquatra.me/posts/acm_mm_22_publications.html" />
    <meta property="og:description" content="Moreno La Quatra personal website" />
    <meta property="og:type" content="website" />
    <link rel="stylesheet" href="../src/style.css" />
    <link rel="stylesheet" href="../prism/prism.css" />
    <script src="../src/jquery-3.6.0.min.js"></script>
</head>

<body id="top" class="libertinus">
    <div data-include="header-nested-1"></div>
    <header>
        <h2 style="text-align: center;"><span class="latex">Benchmarking Representations for Speech, Music, and Acoustic Events</h2>
    </header>

    <img src="resources/sasb2024/arch_logo.png" alt="ARCH logo" width="50%" style="margin: 0 auto; display: block;">

    <div class="abstract">
        <p>
            Audio representation learning (ARL) has seen significant advancements in recent years, with the development of
            various models and datasets aimed at encoding audio signals into meaningful high-level feature representations.
            However, the diversity in ARL methods and datasets poses a challenge in systematically comparing their
            capabilities. To address this issue, we present ARCH (Audio Representations benCHmark), a comprehensive benchmark
            designed to evaluate ARL methods across diverse audio classification domains, covering acoustic events, music, and
            speech.
        </p>
    </div>

    <nav role="navigation" class="toc">
        <ol>
            <li><a href="#plug_and_play">Plug and Play</a></li>
            <li><a href="#extensibility">Extensibility</a></li>
            <li><a href="#standardization">Standardization</a></li>
            <li><a href="#datasets_models">Datasets and Models</a></li>
            <li><a href="#evaluation">Evaluation Procedure</a></li>
            <li><a href="#results">Results and Analysis</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
        </ol>
    </nav>

    <h4 id="plug_and_play"> üîå Plug and Play</h4>
    <p>
        ARCH is designed to be easy to use, providing a unified interface for loading datasets and evaluating audio
        representations. Researchers can readily access the benchmark and start evaluating their models without the need
        for complex setup or configuration.
    </p>

    <hW4 id="extensibility"> üîß Extensibility</h4>
    <p>
        One of the strengths of ARCH lies in its extensibility. Researchers can easily add new datasets, tasks, and
        models to the benchmark, allowing for the evaluation of a wide range of audio representations. This flexibility
        enables the benchmark to adapt to evolving research needs and incorporate new developments in the field.
    </p>

    <h4 id="standardization">‚úÖ Standardization</h4>
    <p>
        With the proliferation of ARL models and datasets, comparing their performance can be challenging. ARCH aims to
        standardize the evaluation of audio representations by providing a common framework for assessment. By
        establishing a standard evaluation protocol, ARCH facilitates fair and consistent comparisons between different
        methods.
    </p>

    <br>
    <br>
    <img src="resources/sasb2024/ARCH_architecture.png" alt="ARCH Architecture" width="80%" style="margin: 0 auto; display: block;">
    <br>
    <br>

    <h4 id="datasets_models"> üõ¢Ô∏è Datasets and Models</h4>
    <p>
        ARCH includes 12 datasets spanning acoustic events, music, and speech domains, each covering single-label and
        multi-label classification tasks. The benchmark evaluates several state-of-the-art SSL models, including Wav2Vec
        2.0, WavLM, HuBERT, data2vec, and XLS-R, pre-trained on diverse datasets such as LibriSpeech, Libri-Light,
        GigaSpeech, and VoxPopuli. Additionally, ARCH introduces new pre-trained models trained on the AudioSet
        collection, addressing the lack of open-source models for non-speech tasks.
    </p>

    <h4 id="evaluation"> üîé Evaluation Procedure</h4>
    <p>
        ARCH follows a standardized evaluation process to assess the quality of learned audio representations. The
        evaluation protocol involves training a simple linear classifier on frame-level representations generated by the
        models and measuring performance metrics such as mean average precision (mAP) for multi-label classification tasks
        and accuracy for single-label tasks.
    </p>

    <h4 id="results"> üìä Results and Analysis</h4>
    <p>
        Extensive experiments conducted on ARCH demonstrate the effectiveness of pre-training models on diverse datasets
        for learning generalizable representations. Models pre-trained on AudioSet consistently outperform speech-pretrained
        models on non-speech tasks, highlighting the importance of diverse pre-training data. Increasing model size also
        leads to performance improvements across all domains, underscoring the benefits of larger model capacities.
        More information on the benchmark and detailed results can be found in the <a href="#">ARCH paper</a> and on
        the associated <a href="https://huggingface.co/spaces/ALM/ARCH">Hugging Face space</a>.
    </p>

    <h4 id="conclusion"> üì¢ Conclusion</h4>
    <p>
        In conclusion, ARCH provides a valuable resource for the standardized evaluation of audio representations across
        diverse domains. By offering a unified framework, ARCH facilitates fair comparisons between different ARL methods,
        helping researchers identify optimal models for specific tasks or domains. With its plug-and-play interface,
        extensibility, and focus on standardization, ARCH aims to accelerate progress in audio representation learning
        and foster collaboration within the research community.
    </p>

    <p>
        To explore the ARCH benchmark and access the results, visit the <a href="https://github.com/MorenoLaQuatra/ARCH">official repository</a>
        or have an overview of the results on the <a href="https://huggingface.co/spaces/ALM/ARCH">ü§ó Hugging Face Space</a>.
    </p>

    <script>
        MathJax = {
            tex: {
                inlineMath: [
                    ['$', '$'],
                ],
            },
        }
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="../prism/prism.js"></script>

    <script async defer data-domain="latex.now.sh" src="https://plausible.io/js/plausible.js"></script>
    <!-- JavaScript Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-A3rJD856KowSb7dwlZdYEkO39Gagi7vIsF0jrRAoQmDKKtQBHUuLZ9AsSv4jD4Xa" crossorigin="anonymous"></script>

    <div data-include="footer"></div>

    <script>
        $(function() {
            var includes = $('[data-include]')
            $.each(includes, function() {
                var file = '../components/' + $(this).data('include') + '.html'
                $(this).load(file)
            })
        })
    </script>


</body>

</html>