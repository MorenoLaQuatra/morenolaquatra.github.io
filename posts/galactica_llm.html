<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google-site-verification" content="" />
    <title>Galactica ü™ê - Moreno La Quatra</title>

    <meta name="description" content="Galactica ü™ê" />
    <meta name="keywords" content="galactica, llm, paperswithcode, 2022, blog" />
    <meta property="og:title" content="Galactica ü™ê" />
    <meta property="og:url" content="https://mlaquatra.me/posts/galactica_llm.html" />
    <meta property="og:description" content="Galactica ü™ê" />
    <meta property="og:type" content="website" />
    <link rel="stylesheet" href="../src/style.css" />
    <link rel="stylesheet" href="../prism/prism.css" />
    <script src="../src/jquery-3.6.0.min.js"></script>
</head>

<body id="top" class="libertinus">
    <div data-include="header-nested-1"></div>
    <header>
        <h1><span class="latex">[DRAFT] Galactica ü™ê - Science LLM (?)</span></h1>
    </header>

    <div class="abstract">
        <p>
            Galactica is a new large language model created by <a href="https://www.paperswithcode.com">Papers with Code</a> and <a href="https://www.meta.com/">Meta</a>. It is trained on a corpus containing scientific papers, knowledge bases, code, etc.
            This post is a quick overview of the model and aims at highlighting the threats and opportunities of such a large model for science.
        </p>
    </div>

    <nav role="navigation" class="toc">
        <ol>
            <li><a href="#another_llm">Just another LLM?</a></li>
            <li><a href="#galactica">Galactica</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#discussion">Discussion</a></li>
        </ol>
    </nav>

    <h4 id="another_llm">Just another LLM?</h4>
    <p>
        Since the introduction of GPT-like models, both the NLP community and the general public have been fascinated by the power of these models. When GPT-3 was released in 2020, it was the first time that the general public got to see a large language model
        in action. We have seen news articles, html pages, and even entire essays generated by GPT-3. The fluency of the generated text was <b>impressive</b> and it was clear that the model was able to fool the human eye.
    </p>
    <p>
        Galactica does not differ much from GPT-3. It is a large language model that comes in different sizes. The largest model has 120 billion parameters and the smallest one has 125 million parameters. The model is, by design, a LLM for <b>science</b>,
        which means that it is trained on a corpus containing scientific papers, knowledge bases, code, etc.
    </p>
    <p>
        So, Galactica is just another LLM? Not really. Galactica is a <b>science LLM</b> and it is trained on a corpus containing scientific papers, knowledge bases, code, etc. This means that Galactica can generate text that sounds scientific. This is
        a very important point because it means that Galactica can generate text that is <b>not always</b> factually correct, but it sounds authoritative and scientific.
    </p>

    <h4 id="galactica">Galactica</h4>

    <p>
        Galactica is an LLM created by <a href="https://www.paperswithcode.com">Papers with Code</a> and <a href="https://www.meta.com/">Meta</a>. It shares most of the characteristics of GPT-3 or OPT models. However, some of the choices made during the
        training of Galactica are quite interesting.
    </p>

    <ol>
        <li>
            <b>Dataset:</b> Galactica is trained on a corpus containing 48 million scientific papers (83% of the total size of the corpus), 2 million code snippets, 8 million documents from reference material, 2 million entries from knowledge bases, and
            other documents both from the web and from the scientific domain. Such a large corpus is a great <b>starting point for a science LLM</b>.
        </li>
        <li>
            <b>Working Memory Token, <code>&lt;work&gt;</code>:</b> reporting directly from the paper
            <blockquote>
                Transformer-based architectures lack an explicit working memory capability, which means a single-forward pass has limited efficacy.
            </blockquote>
            In this case, the authors make explicit reference to the <i>chain-of-thought prompting</i> that is a technique used to enhance the performance of LLMs by providing it with the ability to generate step-by-step process to solve a problem. In
            Galactica, the authors propose to encode this idea directly in the model by adding a specific token to the vocabulary <code>&lt;work&gt;</code>. This token is used to wrap text including the detailed steps to generate a result. This is a very
            interesting idea and it is a great way to <b>explicitly encode</b> the idea of a working memory into the model.
        </li>
        <img src="../imgs/galactica_llm/work_token.png" alt="work token" loading="lazy" />
        <hr>
        <li>
            <b>Citation Token:</b> one of the distinctive features of scientific papers is the presence of citations. In Galactica, the authors propose to encode this idea directly in the model by adding specific tokens to the vocabulary: <code>[START_REF]</code>            and <code>[END_REF]</code>. These tokens are used to wrap text containing a citation. This allows the model to understand both when a citation is present and what is the content of the citation.
        </li>
        <img src="../imgs/galactica_llm/citation_token.png" alt="citation token" loading="lazy" />
        <hr>
        <li>
            <b>Prompt Pre-Training:</b> another important difference with respect to other LLMs is the use of prompt pre-training. In addition to standard <i>next word prediction</i> training, Galactica is trained with prompts. The use of prompts during
            training has been shown to improve the performance of LLMs<label for="sn-1" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Sanh, Victor, et al. "Multitask prompted training enables zero-shot task generalization." - <a href="https://arxiv.org/abs/2110.08207">On arXiv</a></span>.
            Galactica is trained <b>also</b> using prompting during training to boost the performance on downstream tasks (e.g., question answering) and to augment the training data. The authors explicitly mention that the use of prompts during training
            can be beneficial for the LLM but the set of prompts used during training is quite limited. This is a very interesting point because it means that the model can benefit from the use of prompts during training even in limited conditions. In
            the future, it would be interesting to see how the performance of Galactica can be improved by using a larger set of prompts during training.
        </li>
        <img src="../imgs/galactica_llm/prompt_pretraining.png" alt="prompt pretraining" loading="lazy" />
        <hr>
    </ol>

    <p>
        Other standard choices are made during the training of Galactica. For example, the authors use GeLU as activation function, a large 2048 token context window, and learned positional embeddings. The authors also use BPE as tokenization method and they
        use a vocabulary of 50k tokens (!! important to note that the vocabulary is generated using a random sample of 2% of the corpus).
    </p>

    <h4 id="results">Results</h4>

    <p>
        The results of Galactica are quite impressive. The model is tested on a pletora of tasks and it performs very well. Does this mean that Galactica is a good model for science? The choice is yours. Let's wrap up the results with a few takeaways.
    </p>

    <ol>
        <li>
            <b>Multiple steps on the same examples during training</b>: recent work has shown that multiple pre-training epochs on the same (huge!) dataset can be harmful for the performance of LLMs<label for="sn-2" class="sidenote-toggle sidenote-number"></label>
            <span class="sidenote">Hernandez, Danny, et al. "Scaling Laws and Interpretability of Learning from Repeated Data." - <a href="https://arxiv.org/abs/2205.10487">On arXiv</a></span>. In Galactica, the authors investigate this idea by training
            the model for multiple epochs on the same dataset. The main difference with respect to previous work is that the authors use a <i>curated</i> dataset. The results are quite interesting. The model can improve its performance until the 4th epoch
            and then it starts to degrade. This is a very interesting result because it means that the model can benefit from multiple epochs on the same dataset <b>because</b> (i) the dataset is curated or (ii) scientific domain contain dense information
            (more value per token).

            <img src="../imgs/galactica_llm/validation_epochs.png" alt="multiple epochs" loading="lazy" />
        </li>

        <li>
            <b>Latex equations:</b> Galactica is evaluated on the task of generating latex equations. When prompted with a description of a formula, Galactica generates the corresponding latex code.
            <img src="../imgs/galactica_llm/latex_prompt.png" alt="latex" loading="lazy" /> Do you imagine how useful this can be? Imagine that you are writing a paper and you need to include a formula. You can just write a description of the formula
            and Galactica will generate the latex code for you. Sure, the generated latex code can be not correct 100% of the time, but it is a great starting point for your paper.
            <img src="../imgs/galactica_llm/latex_generation_results.png" alt="latex results table" loading="lazy" /> The model is compared with OPT
            <label for="sn-3" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Zhang, Susan, et al. "Opt: Open pre-trained transformer language models." - <a href="https://arxiv.org/abs/2205.01068">On arXiv</a></span>, BLOOM
            <label for="sn-4" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Scao, Teven Le, et al. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." - <a href="https://arxiv.org/abs/2211.05100">On arXiv</a></span>,
            and GPT-3<label for="sn-5" class="sidenote-toggle sidenote-number"></label><span class="sidenote">Brown, Tom, et al. "Language models are few-shot learners." - <a href="https://arxiv.org/abs/2005.14165">On arXiv</a></span>. It obtains the
            best results on all domains. What is more interesting is that <code>Galactica 30B</code> is already able to outperform all competitors in most of the domains.
        </li>

        <li>
            <b>Reasoning:</b> The <code>&lt;work&gt;</code> token is used to wrap text including the detailed steps to finally generate the result. When compared with other LLMs, Galactica is able to more accurately generate the result of a process.
            <img src="../imgs/galactica_llm/reasoning_mmlu.png" alt="reasoning results table" loading="lazy" /> Even in this case the model can outperform all competitors. Again, the <code>Galactica 30B</code> model is already able achieve <b>very good</b>            performance considering the size of the model (tradeoff between size and performance). The introduction of the <code>&lt;work&gt;</code> token can significantly improve the performance of the model and it is a great way to <b>explicitly encode</b>            the idea of a working memory into the model.
        </li>

        <li>
            <b>Scientific NLP</b>: the model is evaluate on knowledge-intensive tasks from multiple domains and knowledge levels (e.g., high school, college, and graduate school).
            <img src="../imgs/galactica_llm/scientific_nlp.png" alt="scientific nlp results table" loading="lazy" />
            <br> A few interesting observations:
            <ol>
                <li>The authors report that the model may be biased towards graduate-level knowledge.</li>
                <li>Considering in-domain performance, the model is able to outperform all competitors</li>
                <li>Considering out-of-domain performance, the model shows good performance overall</li>
                <li>The GPT-3 results are only available for a few tasks, why? I don't know. Even for other competitors the results are not available for all tasks.</li>
            </ol>
        </li>

        <li>
            <b>Citations prediction</b>: here is when the tasks become really interesting. The model is evaluated on the task of predicting the citation of a paper given the left-side context. The model is prompted with a text similar to the following
            one:
            <blockquote>
                FP-growth is a frequent pattern mining algorithm for transactional databases <code>[START_REF]</code>
            </blockquote>
            The model is then asked to predict the citation of the paper in the following way:
            <blockquote>
                FP-growth is a frequent pattern mining algorithm for transactional databases <code>[START_REF]</code>Mining frequent patterns without candidate generation, Han<code>[END_REF]</code>
            </blockquote>

            The model is compared with sparse and dense retrieval models <b>(!! those are the ones we use today for citation lookup).</b>

            <img src="../imgs/galactica_llm/citation_prediction.png" alt="citation prediction results table" loading="lazy" /> One of the possible criticism, in this case, is that the model can be biased towards the most cited papers. That <i>can</i>            be true, but the authors report that the bias is mitigated by the size of the model. It means that the biggest model (e.g., 120B) is able to predict the correct citation even for papers having very few citation. The following image shows how
            the predicted and the actual citation are distributed over the number of citations of the paper.

            <img src="../imgs/galactica_llm/citation_prediction_distribution.png" alt="citation prediction distribution" loading="lazy" />
        </li>
    </ol>

    <p>
        A lot of additional results are reported in the paper. They include biological understanding, drug discovery, chemical understanding and many more. I <b>strongly encourage</b> you to read the paper to learn more about the results.
    </p>

    <h4 id="discussion">Discussion (include personal opinions)</h4>

    <p>
        The paper proposes Galactica as a LLM for <b>science</b>. What are the threats to this claim? The authors <i>advertise</i> Galactica as a model that can be used to generate scientific papers and they provide a public demo to show the impressing
        capabilities of the model. While the results of the model are very interesting, and, for sure the public release of the model is a great achievement, I think that the authors are a bit too optimistic about the capabilities of the model and managed
        to create a hype around the model.
    </p>

    <p>
        <b>Factual correctness</b>: generative models, expecially in the NLP domain, can generate very fluent text but they are not able to guarantee the factual correctness of the generated text. Galactica <b>is not</b> an exception. While the model
        is better than many other LLMs (by design -- and by data!), it can not guarantee that the generated text is correct. The problem of <b>hallucinated</b> content is a very common problem in the NLP domain and the authors are aware of it. They are
        not solving this problem and they are not claiming to solve it. They are just saying that Galactica is better than other LLMs in the scientific. I think that this is a very important point to consider.
    </p>

    <p>
        <b>Misinformation and fake news</b>: the model can be used to generate fake information that really sounds like it is correct. The model is just amazing at "academicese" and it can generate long essays that sound like they are written by a real
        scientist. Is that a problem? The choice is yours. I think that it is not. If the problem is that the model can generate fake information, then we should not use it in that way. Generating fake news is a problem, but it is not a problem of the
        model. It is a problem of the people using the model. The model is just a tool. Let's stop one moment and discuss about this.
    </p>

    <p>
        <b>Academic companion</b>: the best way to think about Galactica is as an academic companion. The model can help you to write a paper, but it <b>must not</b> write a paper for you. You are the researcher and you are the one that knows the domain,
        the problem, the data, the methods, and so on. <b>Non-native</b> speakers can use the model to generate a first draft of a paper or to get suggestion on academic writing. Again, the model can help you to write a paper, but it <b>must not</b> write
        a paper for you. Standing on the shoulders of a giant, I report here a tweet from <a href="https://twitter.com/ylecun" target="_blank">Prof. Yann LeCun</a> that best summarizes my opinion about the model:

        <blockquote class="twitter-tweet">
            <p lang="en" dir="ltr">This tool is to paper writing as driving assistance is to driving.<br>It won&#39;t write papers automatically for you, but it will greatly reduce your cognitive load while you write them. <a href="https://t.co/0WgR8DWUV6">https://t.co/0WgR8DWUV6</a></p>&mdash;
            Yann LeCun (@ylecun) <a href="https://twitter.com/ylecun/status/1592677381818691584?ref_src=twsrc%5Etfw">November 16, 2022</a></blockquote>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    </p>


    <script>
        MathJax = {
            tex: {
                inlineMath: [
                    ['$', '$'],
                ],
            },
        }
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="../prism/prism.js"></script>

    <script async defer data-domain="latex.now.sh" src="https://plausible.io/js/plausible.js"></script>
    <!-- JavaScript Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-A3rJD856KowSb7dwlZdYEkO39Gagi7vIsF0jrRAoQmDKKtQBHUuLZ9AsSv4jD4Xa" crossorigin="anonymous"></script>

    <div data-include="footer"></div>

    <script>
        $(function() {
            var includes = $('[data-include]')
            $.each(includes, function() {
                var file = '../components/' + $(this).data('include') + '.html'
                $(this).load(file)
            })
        })
    </script>


</body>

</html>